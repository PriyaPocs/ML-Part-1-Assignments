{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd17782f-469c-4342-a112-a143e0b0b6fc",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "Answer(Q1):\n",
    "\n",
    "Min-Max scaling is a data preprocessing technique used to transform the values of numeric features within a specific range. The goal of Min-Max scaling is to scale the data to a fixed range, usually between 0 and 1, preserving the relative relationships between the data points.\n",
    "\n",
    "The formula to apply Min-Max scaling to a feature \"X\" is as follows:\n",
    "\n",
    "X_scaled = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "Where:\n",
    "- X_scaled is the scaled value of \"X\" between 0 and 1.\n",
    "- X is the original value of the data point.\n",
    "- X_min is the minimum value of the feature \"X\" in the dataset.\n",
    "- X_max is the maximum value of the feature \"X\" in the dataset.\n",
    "\n",
    "Min-Max scaling is particularly useful when you have features with different ranges and you want to bring them all to a common scale for better analysis, visualization, and modeling. It is sensitive to outliers, as extreme values can impact the scaling, so it's a good idea to handle outliers beforehand, for example, by using outlier detection techniques or other normalization methods like Z-score scaling.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose we have a dataset with one feature, \"Age,\" which represents the age of individuals. The original age values range from 25 to 70. We want to apply Min-Max scaling to bring these values into a range between 0 and 1.\n",
    "\n",
    "Original Age values:   [25, 30, 40, 50, 60, 70]\n",
    "\n",
    "To apply Min-Max scaling, we first find the minimum and maximum values of the Age feature in the dataset:\n",
    "\n",
    "X_min = 25\n",
    "X_max = 70\n",
    "\n",
    "Now, we can calculate the scaled values using the formula:\n",
    "\n",
    "X_scaled = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "Applying the formula to each original age value:\n",
    "\n",
    "X_scaled = (25 - 25)/(70 - 25) = 0\n",
    "\n",
    "X_scaled = (30 - 25)/(70 - 25) = 0.083\n",
    "\n",
    "X_scaled = (40 - 25)/(70 - 25) = 0.333\n",
    "\n",
    "X_scaled = (50 - 25)/(70 - 25} = 0.583\n",
    "\n",
    "X_scaled = (60 - 25)/(70 - 25} = 0.833\n",
    "\n",
    "X_scaled = (70 - 25)/(70 - 25} = 1\n",
    "\n",
    "\n",
    "After applying Min-Max scaling, the scaled age values will be:   [0, 0.083, 0.333, 0.583, 0.833, 1]\n",
    "\n",
    "Now, all the age values are scaled between 0 and 1, making it easier to compare and analyze the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d4827e-6788-4664-ab1f-5f36b27e71ff",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "\n",
    "Answer(Q2):\n",
    "\n",
    "The Unit Vector technique, also known as Vector normalization, is another data preprocessing technique used for feature scaling. Unlike Min-Max scaling, which scales the features to a fixed range (usually between 0 and 1), Unit Vector scaling scales each feature to have a magnitude of 1, effectively creating a unit vector.\n",
    "\n",
    "The formula to apply Unit Vector scaling to a feature \"X\" is as follows:\n",
    "X_scaled = X/|X|\n",
    "\n",
    "Where:\n",
    "- X_scaled is the scaled value of \"X\" with a magnitude of 1.\n",
    "- X is the original value of the data point.\n",
    "- |X| represents the magnitude or Euclidean norm of the feature \"X,\" calculated as sqrt(X_1^2 + X_2^2 + ... + X_n^2) \n",
    "where  X_1, X_2, \\ldots, X_n are the individual values of the feature \"X.\"\n",
    "\n",
    "Unit Vector scaling is useful when you want to bring all the features to the same scale while preserving the direction or angles between data points. It is commonly used in machine learning algorithms that rely on distance metrics (e.g., k-nearest neighbors) to prevent features with large magnitudes from dominating the distance calculations.\n",
    "\n",
    "Now, let's illustrate the application of Unit Vector scaling with an example:\n",
    "\n",
    "Suppose we have a dataset with two features, \"Height\" and \"Weight,\" representing the physical characteristics of individuals. We want to apply Unit Vector scaling to these features.\n",
    "\n",
    "Original data:\n",
    "\n",
    "| Height (cm) | Weight (kg) |\n",
    "|-------------|-------------|\n",
    "| 170         | 65          |\n",
    "| 155         | 50          |\n",
    "| 180         | 75          |\n",
    "| 160         | 55          |\n",
    "| 190         | 85          |\n",
    "\n",
    "To apply Unit Vector scaling, we first need to calculate the magnitude of each data point using the Euclidean norm:\n",
    "\n",
    "For the first data point (170 cm, 65 kg):\n",
    "|X_1| = sqrt(170^2 + 65^2) = sqrt(28900) is approx equal to  169.99\n",
    "\n",
    "Similarly, we calculate the magnitudes for the other data points.\n",
    "\n",
    "Next, we can apply the formula to each data point to get the scaled values:\n",
    "\n",
    "For the first data point:\n",
    "X_scaled, Height => 170/|X_1| is approx equal to 170/169.99 is approx equal to  1\n",
    "\n",
    "X_scaled, Weight => 65/|X_1| => 65/169.99 is approx equal to   0.382 \n",
    "\n",
    "\n",
    "Similarly, we calculate the scaled values for the other data points.\n",
    "\n",
    "Scaled data:\n",
    "\n",
    "| Scaled Height | Scaled Weight |\n",
    "|---------------|---------------|\n",
    "| 1             | 0.382         |\n",
    "| 0.912         | 0.365         |\n",
    "| 1             | 0.417         |\n",
    "| 0.916         | 0.396         |\n",
    "| 1             | 0.447         |\n",
    "\n",
    "Now, all the features are scaled to have a magnitude of 1 while preserving the relative relationships between the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0109649-9522-4a2e-90d5-cdf972a7245c",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Answer(Q3):\n",
    "\n",
    "PCA, which stands for Principal Component Analysis, is a popular dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional space. The main objective of PCA is to find new orthogonal (uncorrelated) features, called principal components, that capture the most significant variance in the data. By retaining only a subset of the principal components, we can effectively reduce the dimensionality of the dataset while preserving the essential information.\n",
    "\n",
    "The steps involved in performing PCA are as follows:\n",
    "\n",
    "1. Standardize the data: Scale the data to have zero mean and unit variance, which is essential to ensure that all features are treated equally during the PCA computation.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix from the standardized data to understand the relationships between the different features.\n",
    "\n",
    "3. Calculate eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Sort the eigenvectors: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The principal components with higher eigenvalues explain more variance and are, therefore, more important.\n",
    "\n",
    "5. Choose the desired number of components: Select a subset of the sorted eigenvectors (principal components) that capture a significant portion of the total variance. This selection determines the reduced dimensionality of the transformed data.\n",
    "\n",
    "6. Transform the data: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "PCA is widely used in various fields, such as image processing, data compression, and feature engineering, to reduce the dimensionality of high-dimensional datasets and improve computational efficiency while preserving important patterns and structures in the data.\n",
    "\n",
    "Now, let's illustrate the application of PCA with a simple example:\n",
    "\n",
    "Suppose we have a dataset with two features, \"Height\" and \"Weight,\" representing the physical characteristics of individuals. We want to apply PCA to reduce the dimensionality of this dataset from 2D to 1D.\n",
    "\n",
    "Original data:\n",
    "\n",
    "| Height (cm) | Weight (kg) |\n",
    "|-------------|-------------|\n",
    "| 170         | 65          |\n",
    "| 155         | 50          |\n",
    "| 180         | 75          |\n",
    "| 160         | 55          |\n",
    "| 190         | 85          |\n",
    "\n",
    "Step 1: Standardize the data (subtract the mean and divide by the standard deviation for each feature).\n",
    "\n",
    "Step 2: Compute the covariance matrix.\n",
    "\n",
    "Step 3: Calculate eigenvectors and eigenvalues.\n",
    "\n",
    "Step 4: Sort the eigenvectors based on their eigenvalues.\n",
    "\n",
    "Suppose the sorted eigenvector matrix looks like this:\n",
    "\\[ \\begin{bmatrix} 0.707 & -0.707 \\\\ 0.707 & 0.707 \\end{bmatrix} \\]\n",
    "\n",
    "Step 5: Choose the desired number of components. In this case, we want to reduce the dimensionality to 1, so we select the first principal component, which corresponds to the eigenvector with the highest eigenvalue.\n",
    "\n",
    "Step 6: Transform the data by projecting it onto the selected principal component.\n",
    "\n",
    "The transformed data (reduced to 1D) will be:\n",
    "\n",
    "| Transformed Data |\n",
    "|------------------|\n",
    "| 1.05             |\n",
    "| -2.12            |\n",
    "| 3.54             |\n",
    "| -1.41            |\n",
    "| 4.24             |\n",
    "\n",
    "Now, the data has been reduced to one dimension (one principal component) while retaining the most significant variance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3591216-8b70-4fba-8e60-b45f1d30d7d0",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "\n",
    "Answer(Q4):\n",
    "\n",
    "PCA (Principal Component Analysis) is a dimensionality reduction technique, and feature extraction is a process of transforming the original features into a new set of features representing the data in a more meaningful or compressed way. The relationship between PCA and feature extraction lies in the fact that PCA can be used as a method for feature extraction to obtain a reduced set of features while retaining the essential information and patterns present in the data.\n",
    "\n",
    "When PCA is used for feature extraction, it transforms the original features into a new set of uncorrelated features called principal components. These principal components are linear combinations of the original features, and they capture the most significant variance in the data. By selecting a subset of these principal components, we can effectively extract the most important information from the original features, resulting in a lower-dimensional representation of the data.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Suppose we have a dataset with four features, \"Feature1,\" \"Feature2,\" \"Feature3,\" and \"Feature4,\" representing some measurements. The dataset is high-dimensional, and we want to perform feature extraction using PCA to reduce the dimensionality.\n",
    "\n",
    "Original data:\n",
    "\n",
    "| Feature1 | Feature2 | Feature3 | Feature4 |\n",
    "|----------|----------|----------|----------|\n",
    "| 2.5      | 0.7      | 1.2      | 3.8      |\n",
    "| 0.3      | 2.8      | 1.0      | 2.5      |\n",
    "| 2.8      | 0.5      | 2.2      | 4.1      |\n",
    "| 1.8      | 1.0      | 0.9      | 3.3      |\n",
    "| 0.5      | 3.4      | 2.0      | 3.6      |\n",
    "\n",
    "Step 1: Standardize the data (subtract the mean and divide by the standard deviation for each feature).\n",
    "\n",
    "Step 2: Compute the covariance matrix.\n",
    "\n",
    "Step 3: Calculate eigenvectors and eigenvalues.\n",
    "\n",
    "Step 4: Sort the eigenvectors based on their eigenvalues.\n",
    "\n",
    "Suppose the sorted eigenvector matrix looks like this:\n",
    "\\[ \\begin{bmatrix} 0.54 & 0.59 & 0.59 & 0.11 \\\\ 0.58 & -0.57 & -0.57 & 0.16 \\\\ 0.58 & -0.57 & 0.58 & -0.11 \\\\ 0.20 & 0.07 & -0.07 & -0.97 \\end{bmatrix} \\]\n",
    "\n",
    "Step 5: Choose the desired number of components. In this case, let's say we want to reduce the dimensionality to 2, so we select the first two principal components, which correspond to the two eigenvectors with the highest eigenvalues.\n",
    "\n",
    "Step 6: Transform the data by projecting it onto the selected principal components.\n",
    "\n",
    "The transformed data (reduced to 2D) will be:\n",
    "\n",
    "| Transformed Feature1 | Transformed Feature2 |\n",
    "|----------------------|----------------------|\n",
    "| 3.29                 | -0.13                |\n",
    "| 2.44                 | 1.55                 |\n",
    "| 3.40                 | -0.55                |\n",
    "| 2.76                 | 0.11                 |\n",
    "| 4.36                 | -1.98                |\n",
    "\n",
    "Now, the data has been reduced to two dimensions, which are the two principal components representing the most significant variance in the original data. These transformed features can be used as a lower-dimensional representation of the original data for further analysis, visualization, or modeling tasks. The new features obtained through PCA are uncorrelated and can provide insights into the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f6483-a8e4-432e-9ffc-4b9ce97b36c8",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "\n",
    "Answer(Q5):\n",
    "\n",
    "\n",
    "To preprocess the data for building a recommendation system for a food delivery service, we can use Min-Max scaling to scale the features \"price,\" \"rating,\" and \"delivery time\" to a common range, usually between 0 and 1. Min-Max scaling will bring all these features to the same scale, making them directly comparable and ensuring that no feature dominates the others due to differences in their original ranges.\n",
    "\n",
    "Here's a step-by-step explanation of how to use Min-Max scaling to preprocess the data:\n",
    "\n",
    "1. Understand the data: Take a look at the dataset and identify the features that need to be scaled. In this case, the features \"price,\" \"rating,\" and \"delivery time\" need to be scaled.\n",
    "\n",
    "2. Calculate the minimum and maximum values for each feature: Find the minimum and maximum values for each of the three features \"price,\" \"rating,\" and \"delivery time\" in the dataset.\n",
    "\n",
    "3. Apply Min-Max scaling formula: Use the Min-Max scaling formula to scale each data point for each feature \"X\":\n",
    "\n",
    "X_scaled = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "Where:\n",
    "- X_scaled is the scaled value of \"X\" between 0 and 1.\n",
    "- X is the original value of the data point.\n",
    "- X_min is the minimum value of the feature \"X\" in the dataset.\n",
    "- X_max is the maximum value of the feature \"X\" in the dataset.\n",
    "\n",
    "4. Perform Min-Max scaling on the dataset: Apply the Min-Max scaling formula to each data point in the \"price,\" \"rating,\" and \"delivery time\" columns separately.\n",
    "\n",
    "For example, let's say we have the following dataset:\n",
    "\n",
    "| Price ($) | Rating (out of 5) | Delivery Time (minutes) |\n",
    "|-----------|-------------------|-------------------------|\n",
    "| 10        | 4.5               | 25                      |\n",
    "| 20        | 3.8               | 30                      |\n",
    "| 15        | 4.0               | 20                      |\n",
    "| 25        | 4.9               | 35                      |\n",
    "| 30        | 4.2               | 40                      |\n",
    "\n",
    "Step 2: Calculate the minimum and maximum values for each feature:\n",
    "\n",
    "X min, Price=10(minimum price)\n",
    "\n",
    "X max, Price=30(maximum price)\n",
    "\n",
    "X min, Rating=3.8(minimum rating)\n",
    "\n",
    "X max, Rating=4.9(maximum rating)\n",
    "\n",
    "X min, Delivery Time=20(minimum delivery time)\n",
    "\n",
    "X max, Delivery Time=40(maximum delivery time)\n",
    "\n",
    "\n",
    "Step 3: Apply Min-Max scaling formula to each data point:\n",
    "\n",
    "For the first data point (10 USD, 4.5 rating, 25 minutes delivery time):\n",
    "\n",
    "X scaled, Price= (30−10)/(10−10)=0\n",
    "\n",
    "X scaled, Rating= (4.9−3.8)/(4.5−3.8)≈0.840\n",
    "\n",
    "X scaled, Delivery Time= (40−20)/(25−20)=0.25\n",
    "\n",
    "Similarly, calculate the scaled values for the other data points.\n",
    "\n",
    "The preprocessed dataset after Min-Max scaling would look like:\n",
    "\n",
    "| Scaled Price | Scaled Rating | Scaled Delivery Time |\n",
    "|--------------|---------------|---------------------|\n",
    "| 0            | 0.840         | 0.25                |\n",
    "| 0.333        | 0             | 0.375               |\n",
    "| 0.167        | 0.267         | 0                   |\n",
    "| 1            | 1             | 0.5                 |\n",
    "| 1            | 0.400         | 1                   |\n",
    "\n",
    "Now, all three features (\"price,\" \"rating,\" and \"delivery time\") are scaled between 0 and 1, making them comparable and ready to be used in building the recommendation system for the food delivery service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9f174-92c1-4920-9e28-932b578d8531",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "\n",
    "Answer(Q6):\n",
    "\n",
    "When building a model to predict stock prices, the dataset may contain a large number of features, which can lead to the curse of dimensionality and potentially affect the performance and interpretability of the model. In such cases, PCA can be used to reduce the dimensionality of the dataset while preserving most of the important information and patterns in the data.\n",
    "\n",
    "Here's a step-by-step explanation of how to use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "1. Standardize the data: Start by standardizing the data to have zero mean and unit variance for each feature. This step is crucial to ensure that all features are treated equally during the PCA computation.\n",
    "\n",
    "2. Compute the covariance matrix: Calculate the covariance matrix from the standardized data. The covariance matrix captures the relationships between the different features and is essential for PCA.\n",
    "\n",
    "3. Calculate eigenvectors and eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components, and the corresponding eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "4. Sort the eigenvectors: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The principal components with higher eigenvalues explain more variance and are, therefore, more important.\n",
    "\n",
    "5. Choose the desired number of components: Decide on the number of principal components you want to keep. This decision can be based on the cumulative explained variance or some predefined threshold. Selecting a smaller number of components will reduce the dimensionality of the dataset.\n",
    "\n",
    "6. Transform the data: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "By selecting a subset of the principal components, we effectively reduce the dimensionality of the dataset. The transformed features (principal components) are uncorrelated and capture the most significant variance in the data, which can be used as input features for building the stock price prediction model.\n",
    "\n",
    "It's important to note that PCA may not always be the best choice for dimensionality reduction in every scenario. It depends on the nature of the data and the specific problem at hand. Sometimes, other dimensionality reduction techniques or feature selection methods may be more appropriate. Therefore, it is advisable to experiment with different approaches and evaluate their impact on the model's performance. Additionally, since stock price prediction is a complex and challenging task, it's essential to consider various other factors, such as time series analysis, market sentiment, and other domain-specific features, while building a robust predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918bdbf9-d928-4033-b6db-164756bfd419",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "Answer(Q7):\n",
    "\n",
    "To perform Min-Max scaling and transform the values to a range of -1 to 1, we need to follow these steps:\n",
    "\n",
    "1. Calculate the minimum and maximum values of the original dataset.\n",
    "2. Apply the Min-Max scaling formula to each value to scale them within the desired range.\n",
    "\n",
    "The Min-Max scaling formula for scaling a value \\(X\\) to a range between \\(a\\) and \\(b\\) is as follows:\n",
    "X_scaled = (X - X_min)/(X_max - X_min)\n",
    "\n",
    "Where:\n",
    "- X_scaled is the scaled value of \"X\" between 0 and 1.\n",
    "- X is the original value of the data point.\n",
    "- X_min is the minimum value of the feature \"X\" in the dataset.\n",
    "- X_max is the maximum value of the feature \"X\" in the dataset.\n",
    "- a is the lower bound of the desired range (-1 in this case).\n",
    "- b is the upper bound of the desired range (1 in this case).\n",
    "\n",
    "Let's apply Min-Max scaling to the given dataset [1, 5, 10, 15, 20] to transform the values to a range of -1 to 1:\n",
    "\n",
    "Step 1: Calculate the minimum and maximum values of the dataset.\n",
    "X_min = 1 \n",
    "X_max}} = 20\n",
    "\n",
    "Step 2: Apply the Min-Max scaling formula to each value:\n",
    "X scaled, 1 = (1−1)/(20−1)×(1−(−1))+(−1)=0\n",
    "\n",
    "X scaled, 5 = (5−1)/(20−1)×(1−(−1))+(−1)≈−0.6\n",
    "\n",
    "X scaled, 10 = (10−1)/(20−1)×(1−(−1))+(−1)≈−0.2\n",
    "\n",
    "X scaled, 15 = (15−1)/(20−1)×(1−(−1))+(−1)≈0.2\n",
    "\n",
    "X scaled, 20 = (20−1)/(20−1)×(1−(−1))+(−1)≈1\n",
    "\n",
    "\n",
    "The scaled values within the range of -1 to 1 will be: [-1, -0.6, -0.2, 0.2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904a8e3-9226-4f3f-b602-996009df666a",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "\n",
    "Answer(Q8):\n",
    "\n",
    "Performing feature extraction using PCA involves transforming the original features into a new set of uncorrelated features called principal components. The number of principal components to retain is a crucial decision as it determines the reduced dimensionality of the dataset. The goal is to retain enough principal components to capture a significant portion of the variance in the data while reducing the dimensionality as much as possible.\n",
    "\n",
    "To decide on the number of principal components to retain, we can consider the cumulative explained variance. It is the cumulative sum of the eigenvalues (variance) of the sorted principal components. By plotting the cumulative explained variance against the number of principal components, we can visually identify the point where the curve starts to level off. This point represents the number of principal components that explain most of the variance in the data.\n",
    "\n",
    "Let's assume we have a dataset with the following features: [height, weight, age, gender, blood pressure]. The first step is to preprocess the data by standardizing the features to have zero mean and unit variance. Then, we compute the covariance matrix and calculate the eigenvectors and eigenvalues.\n",
    "\n",
    "Next, we sort the eigenvectors based on their corresponding eigenvalues in descending order. After that, we calculate the cumulative explained variance. If we plot the cumulative explained variance against the number of principal components, the point where the curve starts to level off can guide us in choosing the number of components to retain.\n",
    "\n",
    "For example, let's say the cumulative explained variance plot looks like this:\n",
    "\n",
    "| Number of Components | Cumulative Explained Variance |\n",
    "|---------------------|------------------------------|\n",
    "| 1                   | 0.60                         |\n",
    "| 2                   | 0.80                         |\n",
    "| 3                   | 0.90                         |\n",
    "| 4                   | 0.95                         |\n",
    "| 5                   | 1.00                         |\n",
    "\n",
    "In this case, we see that using just one principal component explains 60% of the variance, two components explain 80%, three components explain 90%, and four components explain 95%. The cumulative explained variance levels off after four components.\n",
    "\n",
    "Based on this analysis, we could choose to retain four principal components. These four components capture 95% of the variance in the data, which is a substantial portion. Retaining only four components helps reduce the dimensionality significantly while preserving most of the information necessary for our modeling task. By choosing four components, we strike a good balance between reducing dimensionality and retaining important patterns in the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
